# -*- coding: utf-8 -*-
from pathlib import Path
import os, re, json, time, random, threading, socket, shutil
from queue import Queue
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
import pandas as pd
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# =========================================================
# Fixed Paths
# =========================================================
BASE_DIR = Path.cwd()
DATA_DIR = BASE_DIR / "data"
INPUT_DIR = DATA_DIR / "input"
OUTPUT_DIR = DATA_DIR / "output"

os.makedirs(INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =========================================================
# Get SESSION_DIR from environment or use OUTPUT_DIR
# =========================================================
SESSION_DIR = Path(os.getenv("SESSION_DIR", str(OUTPUT_DIR)))

# =========================================================
# Gemini SDK Import
# =========================================================
try:
    import google.genai as genai
    from google.genai import types
    print("‚úÖ Gemini SDK loaded successfully (google-genai).")
except ImportError:
    try:
        import google.generativeai as genai
        from google.generativeai import types
        print("‚ö†Ô∏è Using legacy google-generativeai SDK.")
    except Exception as e:
        print("‚ùå Gemini SDK not installed properly:", e)
        import sys
        sys.exit(1)

# =========================================================
# File Paths
# =========================================================
SOURCE_FOLDER = INPUT_DIR
RENAMED_DIR = DATA_DIR / "renamed"

MIX_OCR_QR_JSON = OUTPUT_DIR / "mix_ocr_qr.json"
OUT_JSON = OUTPUT_DIR / "gemini_scrap_output.json"
CLEAN_URLS = OUTPUT_DIR / "urls_clean.json"
WEB_ANALYSIS_XLSX = OUTPUT_DIR / "web_analysis.xlsx"
TEMP_EXCEL = OUTPUT_DIR / "web_analysis.tmp.xlsx"

os.makedirs(SOURCE_FOLDER, exist_ok=True)
os.makedirs(RENAMED_DIR, exist_ok=True)

# =========================================================
# Configuration
# =========================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyAhuC9Grg_FlxwDwYUW-_CpNaFzjwUg24w")
MODEL_NAME = "gemini-2.0-flash-exp"

THREAD_COUNT = 5
MAX_DEPTH = 2
MAX_PAGES_PER_SITE = 25
REQUEST_TIMEOUT = (8, 20)
SLEEP_BETWEEN = (0.8, 2.0)
MAX_RETRIES_HTTP = 3
MAX_RETRIES_GEMINI = 3
CHECK_DOMAIN_EXISTENCE = True

IRANIAN_TLDS = ['.ir', '.ac.ir', '.co.ir', '.org.ir', '.gov.ir', '.id.ir', '.net.ir']

client = genai.Client(api_key=GOOGLE_API_KEY)
lock = threading.Lock()

# =========================================================
# Fields & Prompts
# =========================================================
FIELDS = [
    "CompanyNameEN", "CompanyNameFA", "Logo", "Industry", "Certifications",
    "ContactName", "PositionEN", "PositionFA", "Department",
    "Phone1", "Phone2", "Fax", "WhatsApp", "Telegram", "Instagram", "LinkedIn",
    "Website", "Email", "OtherEmails",
    "AddressEN", "AddressFA", "Country", "City",
    "ProductName", "ProductCategory", "ProductDescription", "Applications",
    "Brands", "Description", "History", "Employees", "ClientsPartners", "Markets"
]

TRANSLATABLE_FIELDS = [
    ("CompanyNameEN", "CompanyNameFA_translated"),
    ("AddressEN", "AddressFA_translated"),
    ("ProductName", "ProductNameFA"),
    ("ProductCategory", "ProductCategoryFA"),
    ("ProductDescription", "ProductDescriptionFA"),
    ("Applications", "ApplicationsFA"),
    ("Description", "DescriptionFA"),
    ("History", "HistoryFA"),
    ("Employees", "EmployeesFA"),
    ("ClientsPartners", "ClientsPartnersFA"),
]

PROMPT_EXTRACT = """
You are a bilingual (Persian-English) company information extractor.
Extract the following JSON fields from the provided website text.
Return ONLY strict JSON object. If a field has no value, return null.

Fields:
{fields}

Website text (mixed FA/EN):
---
{text}
---
"""

PROMPT_TRANSLATE_EN2FA = """
Translate the following English fields into formal Persian.
Return ONLY valid JSON with the same keys and Persian values. Do NOT add extra text.

Fields JSON:
{json_chunk}
"""

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
}

# =========================================================
# Utility Functions
# =========================================================
def normalize_root(url: str) -> str:
    u = url.strip()
    if not re.match(r"^https?://", u, re.I):
        u = "https://" + u
    p = urlparse(u)
    return f"{p.scheme}://{p.netloc}".lower()

def is_iranian_domain(url: str) -> bool:
    try:
        netloc = urlparse(normalize_root(url)).netloc.lower()
        return any(netloc.endswith(tld) for tld in IRANIAN_TLDS)
    except:
        return False

def domain_exists(url: str) -> bool:
    try:
        host = urlparse(normalize_root(url)).netloc
        socket.setdefaulttimeout(5)
        socket.gethostbyname(host)
        return True
    except Exception:
        return False

# =========================================================
# Extract URLs
# =========================================================
def extract_urls_from_mix(input_path: str, output_path: str):
    print("üåê Extracting all URLs from mix_ocr_qr.json...")
    try:
        raw = json.loads(Path(input_path).read_text(encoding="utf-8"))
    except Exception as e:
        print(f"‚ùå Error reading input JSON: {e}")
        return []

    urls = set()
    SOCIAL_EXCLUDE = ("instagram.com", "linkedin.com", "twitter.com", "x.com",
                      "facebook.com", "t.me", "wa.me", "youtube.com", "gmail.com", "mail.")
    url_pattern = re.compile(r"(https?://[^\s\"'<>]+|www\.[^\s\"'<>]+)", re.I)

    def collect(obj):
        if isinstance(obj, str):
            for m in url_pattern.findall(obj):
                u = m.strip().rstrip(".,)")
                if any(u.lower().endswith(ext) for ext in 
                       [".jpg", ".jpeg", ".png", ".gif", ".svg", ".pdf", ".zip", ".rar", ".xls", ".xlsx"]):
                    continue
                if not u.lower().startswith("http"):
                    u = "https://" + u
                r = normalize_root(u)
                if any(s in r for s in SOCIAL_EXCLUDE): continue
                urls.add(r)
        elif isinstance(obj, list):
            for v in obj: collect(v)
        elif isinstance(obj, dict):
            for k, v in obj.items():
                if k == "raw_excel_data":
                    sheets = v.get("sheets", [])
                    for sh in sheets:
                        for row in sh.get("data", []):
                            for val in row.values():
                                collect(val)
                else:
                    collect(v)

    collect(raw)
    roots = sorted(urls)
    if CHECK_DOMAIN_EXISTENCE:
        roots = [u for u in roots if domain_exists(u)]

    Path(output_path).write_text(json.dumps(roots, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"‚úÖ Found {len(roots)} clean root URLs ‚Üí {output_path}")
    return roots

# =========================================================
# Web Crawling & Cleaning (FIXED SSL)
# =========================================================
def fetch(url: str) -> tuple[str, str]:
    verify_ssl = not is_iranian_domain(url)
    ssl_status = "üîí SSL ON" if verify_ssl else "üîì SSL OFF (Iranian)"
    
    for i in range(MAX_RETRIES_HTTP):
        try:
            print(f"  üîÑ Attempt {i+1}/{MAX_RETRIES_HTTP} [{ssl_status}]: {url}")
            r = requests.get(
                url, 
                headers=HEADERS, 
                timeout=REQUEST_TIMEOUT, 
                verify=verify_ssl,
                allow_redirects=True
            )
            if r.status_code == 200:
                print(f"  ‚úÖ Success: {url}")
                return (r.text, "")
            else:
                print(f"  ‚ö†Ô∏è Status {r.status_code}: {url}")
                if i == MAX_RETRIES_HTTP - 1:
                    return ("", f"HTTP_{r.status_code}")
        except requests.exceptions.SSLError as e:
            if verify_ssl and i == 0:
                print(f"  üîÑ SSL Error, retrying without verification: {url}")
                try:
                    r = requests.get(
                        url, 
                        headers=HEADERS, 
                        timeout=REQUEST_TIMEOUT, 
                        verify=False,
                        allow_redirects=True
                    )
                    if r.status_code == 200:
                        print(f"  ‚úÖ Success (SSL disabled): {url}")
                        return (r.text, "")
                except:
                    pass
            print(f"  üîê SSL Error: {url}")
            if i == MAX_RETRIES_HTTP - 1:
                return ("", "SSL_ERROR")
        except requests.exceptions.Timeout:
            print(f"  ‚è∞ Timeout: {url}")
            if i == MAX_RETRIES_HTTP - 1:
                return ("", "TIMEOUT")
        except requests.exceptions.ConnectionError:
            print(f"  üîå Connection Error: {url}")
            if i == MAX_RETRIES_HTTP - 1:
                return ("", "CONNECTION_ERROR")
        except Exception as e:
            print(f"  ‚ùå Error: {url} -> {str(e)[:100]}")
            if i == MAX_RETRIES_HTTP - 1:
                return ("", f"ERROR: {str(e)[:50]}")
        
        time.sleep(2.0 * (i + 1))
    
    return ("", "MAX_RETRIES_EXCEEDED")

def clean_text(html: str) -> str:
    if not html: return ""
    soup = BeautifulSoup(html, "html.parser")
    for t in soup(["script","style","noscript","iframe","svg"]): t.extract()
    text = soup.get_text(" ", strip=True)
    return re.sub(r"\s+", " ", text).strip()

def crawl_site(root: str, max_depth=MAX_DEPTH, max_pages=MAX_PAGES_PER_SITE) -> tuple[str, str]:
    print(f"\nüï∑Ô∏è Starting crawl: {root}")
    seen = set()
    q = [(root, 0)]
    texts = []
    errors = []
    
    while q and len(seen) < max_pages:
        url, d = q.pop(0)
        if url in seen or d > max_depth: continue
        seen.add(url)
        
        html, error = fetch(url)
        
        if error:
            errors.append(f"{url}: {error}")
            continue
            
        txt = clean_text(html)
        if txt:
            texts.append(txt[:40000])
            print(f"  üìÑ Extracted {len(txt)} chars from {url}")
        else:
            errors.append(f"{url}: EMPTY_CONTENT")
        
        if html:
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True):
                nxt = urljoin(root, a["href"])
                if nxt.startswith(root) and nxt not in seen and len(seen) < max_pages:
                    q.append((nxt, d+1))
        
        time.sleep(random.uniform(*SLEEP_BETWEEN))
    
    combined = "\n".join(texts)[:180000]
    
    if not combined:
        error_summary = "; ".join(errors[:3])
        print(f"  ‚ùå No content extracted from {root}")
        return ("", error_summary or "NO_CONTENT")
    
    print(f"  ‚úÖ Total extracted: {len(combined)} chars from {len(texts)} pages")
    return (combined, "")

# =========================================================
# Gemini + Translation
# =========================================================
def gemini_json(prompt: str, schema: dict):
    schema_obj = types.Schema(type=types.Type.OBJECT, properties=schema, required=[])
    for i in range(MAX_RETRIES_GEMINI):
        try:
            resp = client.models.generate_content(
                model=MODEL_NAME,
                contents=[types.Part(text=prompt)],
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    response_mime_type="application/json",
                    response_schema=schema_obj
                )
            )
            return json.loads(resp.text)
        except Exception as e:
            print(f"‚ö†Ô∏è Gemini error (attempt {i+1}): {str(e)[:100]}")
            if i == MAX_RETRIES_GEMINI-1: 
                return {}
            time.sleep(1.5*(i+1))
    return {}

def extract_with_gemini(text: str):
    fields = "\n".join([f"- {f}" for f in FIELDS])
    prompt = PROMPT_EXTRACT.format(fields=fields, text=text)
    schema = {f: types.Schema(type=types.Type.STRING, nullable=True) for f in FIELDS}
    data = gemini_json(prompt, schema)
    return {f: (data.get(f) or "") for f in FIELDS}

def translate_fields(data: dict):
    to_translate = {en: data.get(en) for en, _ in TRANSLATABLE_FIELDS if data.get(en)}
    for en, fa_col in TRANSLATABLE_FIELDS:
        data[fa_col] = ""
    
    if not to_translate:
        return data
    
    prompt = PROMPT_TRANSLATE_EN2FA.format(json_chunk=json.dumps(to_translate, ensure_ascii=False))
    schema = {k: types.Schema(type=types.Type.STRING, nullable=True) for k in to_translate.keys()}
    tr = gemini_json(prompt, schema)
    
    for en, fa_col in TRANSLATABLE_FIELDS:
        data[fa_col] = tr.get(en, "")
    
    return data

# =========================================================
# Worker & Main
# =========================================================
def worker(q: Queue, results: list):
    while True:
        try:
            root = q.get_nowait()
        except:
            break
        
        try:
            print(f"\n{'='*60}")
            print(f"üåê Processing: {root}")
            print(f"{'='*60}")
            
            text, error = crawl_site(root)
            
            if error or not text:
                data = {
                    "url": root, 
                    "error": error or "NO_CONTENT",
                    "status": "FAILED"
                }
                print(f"‚ùå Failed: {root} - {error or 'NO_CONTENT'}")
            else:
                print(f"üß† Analyzing with Gemini: {root}")
                data = extract_with_gemini(text)
                data = translate_fields(data)
                data["url"] = root
                data["status"] = "SUCCESS"
                data["error"] = ""
                print(f"‚úÖ Success: {root}")
                
        except Exception as e:
            data = {
                "url": root, 
                "error": f"EXCEPTION: {str(e)[:100]}",
                "status": "EXCEPTION"
            }
            print(f"‚ùå Exception for {root}: {str(e)[:100]}")
        
        with lock:
            results.append(data)
            try:
                Path(OUT_JSON).write_text(
                    json.dumps(results, ensure_ascii=False, indent=2), 
                    encoding="utf-8"
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to save JSON: {e}")
        
        q.task_done()
        time.sleep(random.uniform(*SLEEP_BETWEEN))

def main():
    print("\n" + "="*60)
    print("üöÄ Starting Web Scraping Process")
    print("="*60 + "\n")
    
    roots = extract_urls_from_mix(str(MIX_OCR_QR_JSON), str(CLEAN_URLS))
    if not roots:
        print("‚ö†Ô∏è No URLs found.")
        return

    results = []
    q = Queue()
    for r in roots: q.put(r)

    threads = []
    for _ in range(min(THREAD_COUNT, len(roots))):
        t = threading.Thread(target=worker, args=(q, results), daemon=True)
        t.start()
        threads.append(t)
    
    for t in threads: t.join()

    print("\n" + "="*60)
    print("üìä Creating Excel Report")
    print("="*60 + "\n")

    df = pd.DataFrame(results)
    
    ordered_cols = ["url", "status", "error"]
    
    for field in FIELDS:
        ordered_cols.append(field)
        for en_field, fa_field in TRANSLATABLE_FIELDS:
            if en_field == field:
                ordered_cols.append(fa_field)
                break
    
    for en_field, fa_field in TRANSLATABLE_FIELDS:
        if en_field not in FIELDS and en_field not in ordered_cols:
            ordered_cols.append(en_field)
            ordered_cols.append(fa_field)
    
    for col in ordered_cols:
        if col not in df.columns:
            df[col] = ""
    
    df = df[ordered_cols]
    
    try:
        df.to_excel(TEMP_EXCEL, index=False)
        shutil.move(str(TEMP_EXCEL), str(WEB_ANALYSIS_XLSX))
        print(f"‚úÖ Excel saved: {WEB_ANALYSIS_XLSX}")
    except Exception as e:
        print(f"‚ùå Failed to save Excel: {e}")
    
    success = len([r for r in results if r.get("status") == "SUCCESS"])
    failed = len(results) - success
    
    print("\n" + "="*60)
    print(f"‚úÖ Success: {success}/{len(results)}")
    print(f"‚ùå Failed: {failed}/{len(results)}")
    print("="*60 + "\n")

def run_web_scraping():
    """Run web scraping"""
    print("üåê Starting web scraping...")
    main()
    return str(WEB_ANALYSIS_XLSX)

if __name__ == "__main__":
    main()