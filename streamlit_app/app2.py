# -*- coding: utf-8 -*-
"""
üéØ smart exhibition pipeline ‚Äî final unified edition + google sheets  
a full merge of the two apps: "ultimate smart exhibition pipeline" + "smart data pipeline"  
- cool ui from version 1 + logic, logging, and quota management from version 2  
- excel mode and ocr/qr mode with auto detection  
- smart metadata injection (exhibition + source + smart position)  
- fast mode, debug mode, rate limiting, daily quota  
- ‚ú® batch processing: images (5), pdfs (4), excel (1)  
- ‚ú® quality control tracking: user name, role, date, time  
- ‚òÅÔ∏è google sheets integration: auto-save data to google drive  

run:  
    streamlit run smart_exhibition_pipeline_final.py

"""

import streamlit as st
import subprocess
import os
import sys
import json
import time
import datetime
from pathlib import Path
import pandas as pd
import numpy as np
import re
import shutil

from supabase import create_client, Client

# =========================================================
# page settings
# =========================================================
st.set_page_config(
    page_title="Smart Exhibition Pipeline",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)


# =========================================================
# cool ui with professional gradients
# =========================================================
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;700&display=swap');
    * { font-family: 'Vazirmatn', sans-serif; }
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 3rem 2rem; border-radius: 20px; text-align: center; margin-bottom: 2rem;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3); animation: slideDown 0.6s ease-out;
    }
    @keyframes slideDown { from { opacity: 0; transform: translateY(-30px);} to { opacity:1; transform: translateY(0);} }
    .main-header h1 { color: white; font-size: 2.8rem; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); }
    .main-header p { color: rgba(255,255,255,0.9); font-size: 1.2rem; margin: 0.5rem 0 0 0; }
    .metric-card {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 2rem; border-radius: 15px; text-align: center; color: white;
        box-shadow: 0 8px 32px rgba(240, 147, 251, 0.3); transition: transform .3s, box-shadow .3s;
        animation: fadeIn .8s ease-out;
    }
    .metric-card:hover { transform: translateY(-5px); box-shadow: 0 12px 48px rgba(240,147,251,.4); }
    @keyframes fadeIn { from { opacity:0; transform: scale(.9);} to { opacity:1; transform: scale(1);} }
    .metric-card h3 { font-size:1rem; margin:0 0 .5rem 0; opacity:.9; }
    .metric-card h2 { font-size:2rem; margin:0; font-weight:bold; }
    .quota-card {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        padding:1.5rem; border-radius:15px; color:white; box-shadow:0 8px 32px rgba(79,172,254,.3); margin-bottom:1rem;
    }
    .quota-number { font-size:3rem; font-weight:bold; margin:.5rem 0; }
    .status-box { padding:1.5rem; border-radius:15px; margin:1rem 0; animation: slideIn .5s ease-out; box-shadow:0 4px 20px rgba(0,0,0,.1); }
    @keyframes slideIn { from { opacity:0; transform: translateX(-20px);} to { opacity:1; transform: translateX(0);} }
    .status-success { background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color:white; }
    .status-warning { background: linear-gradient(135deg, #f7971e 0%, #ffd200 100%); color:#333; }
    .status-error { background: linear-gradient(135deg, #eb3349 0%, #f45c43 100%); color:white; }
    .status-info { background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color:white; }
    .stButton>button {
        width:100%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color:white; border:none; padding:1rem 2rem; font-size:1.1rem; font-weight:bold;
        border-radius:12px; box-shadow:0 6px 24px rgba(102,126,234,.3); transition: all .3s ease;
    }
    .stButton>button:hover { transform: translateY(-2px); box-shadow:0 8px 32px rgba(102,126,234,.4); }
    .stProgress > div > div { background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); }
    .loading-spinner {
        display:inline-block; width:20px; height:20px; border:3px solid rgba(255,255,255,.3);
        border-radius:50%; border-top-color:#fff; animation:spin 1s ease-in-out infinite;
    }
    @keyframes spin { to { transform: rotate(360deg); } }
    .badge {
        display:inline-block; padding:.5rem 1rem; border-radius:20px; font-size:.9rem; font-weight:bold; margin:.2rem;
    }
    .badge-success { background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); color:white; }
    .badge-warning { background: linear-gradient(135deg, #f7971e 0%, #ffd200 100%); color:#333; }
    .badge-error { background: linear-gradient(135deg, #eb3349 0%, #f45c43 100%); color:white; }
    .file-display {
        padding:1rem; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius:10px; margin:.5rem 0;
    }
    .file-display h4 { margin:0; color:#333; }
    .file-display p { margin:.5rem 0 0 0; color:#666; font-size:.9rem; }
    .qc-card {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        padding: 1.5rem; border-radius: 15px; margin-bottom: 1rem;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    .qc-card h4 { color: #333; margin: 0 0 0.5rem 0; }
    .qc-card p { color: #666; margin: 0.25rem 0; font-size: 0.9rem; }
</style>
""", unsafe_allow_html=True)

# =========================================================
# API Keys
# =========================================================
API_KEYS = {
    "excel": "AIzaSyBzVNw....R35hoZNxqsW6pc",
    "ocr": "AIzaSyCKoa.....IBHy1rt61Cl2ZTs",
    "scrap": "AIzaSyAhuC9Grg_.....aFzjwUg24w"
}
for key_name, key_value in API_KEYS.items():
    os.environ[f"GOOGLE_API_KEY_{key_name.upper()}"] = key_value
    os.environ["GOOGLE_API_KEY"] = key_value
    os.environ["GEMINI_API_KEY"] = key_value


# =========================================================
# GOOGLE SHEETS INTEGRATION
# =========================================================
from google.oauth2 import service_account
from googleapiclient.discovery import build

GOOGLE_SCOPES = [
    'https://www.googleapis.com/auth/drive.file',
    'https://www.googleapis.com/auth/spreadsheets'
]

@st.cache_resource
def get_google_services():
    """connect to google drive and sheets"""
    try:
        creds = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=GOOGLE_SCOPES
        )
        drive_service = build('drive', 'v3', credentials=creds)
        sheets_service = build('sheets', 'v4', credentials=creds)
        return drive_service, sheets_service
    except Exception as e:
        st.error(f"‚ùå ÿÆÿ∑ÿß ÿØÿ± ÿßÿ™ÿµÿßŸÑ ÿ®Ÿá Google: {e}")
        return None, None

def _col_index_to_letter(col_index):
    """convert index to excel column letter (0->A, 25->Z, 26->AA)"""
    result = ""
    while col_index >= 0:
        result = chr(col_index % 26 + 65) + result
        col_index = col_index // 26 - 1
    return result

def find_or_create_data_table(drive_service, sheets_service, folder_id=None):
    """find or create a sheet in drive"""
    try:
        table_name = "Exhibition_Data_Table"
        query = f"name='{table_name}' and mimeType='application/vnd.google-apps.spreadsheet' and trashed=false"
        if folder_id:
            query += f" and '{folder_id}' in parents"
        
        results = drive_service.files().list(
            q=query, spaces='drive', fields='files(id, name, webViewLink)', pageSize=1
        ).execute()
        
        files = results.get('files', [])
        
        if files:
            file_id = files[0]['id']
            file_url = files[0].get('webViewLink', f"https://docs.google.com/spreadsheets/d/{file_id}/edit")
            print(f"   ‚úÖ ÿ¨ÿØŸàŸÑ ŸÖŸàÿ¨ŸàÿØ: {file_id}")
            return file_id, file_url, True
        
        print(f"   üìù ÿ≥ÿßÿÆÿ™ ÿ¨ÿØŸàŸÑ ÿ¨ÿØ€åÿØ...")
        spreadsheet = sheets_service.spreadsheets().create(
            body={
                'properties': {'title': table_name},
                'sheets': [{'properties': {'title': 'Data', 'gridProperties': {'frozenRowCount': 1}}}]
            },
            fields='spreadsheetId'
        ).execute()
        
        file_id = spreadsheet.get('spreadsheetId')
        file_url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit"
        
        if folder_id:
            drive_service.files().update(fileId=file_id, addParents=folder_id, fields='id, parents').execute()
        
        print(f"   ‚úÖ ÿ¨ÿØŸàŸÑ ÿ¨ÿØ€åÿØ ÿ≥ÿßÿÆÿ™Ÿá ÿ¥ÿØ: {file_id}")
        return file_id, file_url, False
        
    except Exception as e:
        print(f"   ‚ùå ÿÆÿ∑ÿß: {e}")
        return None, None, False

def append_excel_data_to_sheets(excel_path, folder_id=None):
    """read excel data and append to google sheets (variable number of rows)"""
    try:
        drive_service, sheets_service = get_google_services()
        if not drive_service or not sheets_service:
            return False, "ÿπÿØŸÖ ÿßÿ™ÿµÿßŸÑ ÿ®Ÿá Google", None, 0

        print(f"\n‚òÅÔ∏è ÿ¥ÿ±Ÿàÿπ ÿ∞ÿÆ€åÿ±Ÿá ÿØÿßÿØŸá‚ÄåŸáÿß ÿØÿ± Google Drive...")

        # use your existing sheet (instead of creating a new one)
        file_id = "1OeQbiqvo6v58rcxaoSUidOk0IxSGmL8YCpLnyh27yuE"
        file_url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit"
        exists = True
        print(f"   ‚úÖ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ Google Sheet ŸÖŸàÿ¨ŸàÿØ: {file_url}")

        #file_id, file_url, exists = find_or_create_data_table(drive_service, sheets_service, folder_id)
        if not file_id:
            return False, "ÿÆÿ∑ÿß ÿØÿ± ÿ≥ÿßÿÆÿ™ ÿ¨ÿØŸàŸÑ", None, 0
        
        print(f"üìñ ÿÆŸàÿßŸÜÿØŸÜ ÿØÿßÿØŸá‚ÄåŸáÿß€å Excel: {excel_path.name}")
        df = pd.read_excel(excel_path)
        if df.empty:
            return False, "Excel ÿÆÿßŸÑ€å ÿßÿ≥ÿ™", None, 0
        
        print(f"   ‚úÖ {len(df)} ÿ±ÿØ€åŸÅ √ó {len(df.columns)} ÿ≥ÿ™ŸàŸÜ ÿÆŸàÿßŸÜÿØŸá ÿ¥ÿØ")
        
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).replace('nan', '').replace('None', '').replace('NaT', '')
        
        sheet_name = 'Sheet1'
        
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=file_id, range=f'{sheet_name}!1:1'
        ).execute()
        
        existing_headers = result.get('values', [[]])[0] if result.get('values') else []
        new_headers = df.columns.tolist()
        
        print(f"   üìã ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ŸÖŸàÿ¨ŸàÿØ: {len(existing_headers)} | ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ÿ¨ÿØ€åÿØ: {len(new_headers)}")
        
        if not existing_headers:
            values = [new_headers] + df.values.tolist()
            print(f"   ‚ÑπÔ∏è ÿ¨ÿØŸàŸÑ ÿÆÿßŸÑ€åÿå ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ {len(new_headers)} ÿ≥ÿ™ŸàŸÜ")
        else:
            new_columns = [col for col in new_headers if col not in existing_headers]
            
            all_columns = existing_headers.copy()
            for col in new_columns:
                if col not in all_columns:
                    all_columns.append(col)
            
            print(f"   üìä ÿ™ÿ±ÿ™€åÿ® ŸÜŸáÿß€å€å: {len(all_columns)} ÿ≥ÿ™ŸàŸÜ")
            
            if new_columns:
                print(f"   üÜï ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ÿ¨ÿØ€åÿØ: {new_columns}")
                print(f"   üîÑ ÿ¢ŸæÿØ€åÿ™ ŸáÿØÿ±Ÿáÿß...")
                sheets_service.spreadsheets().values().update(
                    spreadsheetId=file_id,
                    range=f'{sheet_name}!1:1',
                    valueInputOption='USER_ENTERED',
                    body={'values': [all_columns]}
                ).execute()
                
                result = sheets_service.spreadsheets().values().get(
                    spreadsheetId=file_id, range=f'{sheet_name}!A:A'
                ).execute()
                existing_rows_count = len(result.get('values', [])) - 1
                
                if existing_rows_count > 0:
                    print(f"   üìù Ÿæÿ± ⁄©ÿ±ÿØŸÜ {existing_rows_count} ÿ±ÿØ€åŸÅ ŸÇÿØ€åŸÖ€å...")
                    empty_values = [[''] * len(new_columns) for _ in range(existing_rows_count)]
                    start_col_index = len(existing_headers)
                    start_col_letter = _col_index_to_letter(start_col_index)
                    end_col_letter = _col_index_to_letter(start_col_index + len(new_columns) - 1)
                    
                    sheets_service.spreadsheets().values().update(
                        spreadsheetId=file_id,
                        range=f'{sheet_name}!{start_col_letter}2:{end_col_letter}{existing_rows_count+1}',
                        valueInputOption='USER_ENTERED',
                        body={'values': empty_values}
                    ).execute()
                    print(f"   ‚úÖ ÿ±ÿØ€åŸÅ‚ÄåŸáÿß€å ŸÇÿØ€åŸÖ€å ÿ¢ŸæÿØ€åÿ™ ÿ¥ÿØ")
            
            for col in all_columns:
                if col not in df.columns:
                    df[col] = ''
            
            df = df[all_columns]
            print(f"   ‚úÖ DataFrame ŸÖÿ±ÿ™ÿ® ÿ¥ÿØ: {len(df)} ÿ±ÿØ€åŸÅ √ó {len(all_columns)} ÿ≥ÿ™ŸàŸÜ")
            values = df.values.tolist()
        
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=file_id, range=f'{sheet_name}!A:A'
        ).execute()
        existing_rows = len(result.get('values', []))
        
        print(f"   üìä ÿ±ÿØ€åŸÅ ŸÅÿπŸÑ€å: {existing_rows}")
        print(f"   üì§ ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ {len(values)} ÿ±ÿØ€åŸÅ...")
        
        body = {'values': values}
        result = sheets_service.spreadsheets().values().append(
            spreadsheetId=file_id,
            range=f'{sheet_name}!A:A',
            valueInputOption='USER_ENTERED',
            insertDataOption='INSERT_ROWS',
            body=body
        ).execute()
        
        updated_rows = result.get('updates', {}).get('updatedRows', 0)
        total_rows = existing_rows + updated_rows
        
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=file_id, range=f'{sheet_name}!1:1'
        ).execute()
        total_columns = len(result.get('values', [[]])[0])
        
        total_cells = total_rows * total_columns
        capacity = (total_cells / 10_000_000) * 100
        
        print(f"   ‚úÖ {updated_rows} ÿ±ÿØ€åŸÅ ÿ¨ÿØ€åÿØ ÿßÿ∂ÿßŸÅŸá ÿ¥ÿØ")
        print(f"   üìä ÿ¨ŸÖÿπ: {total_rows} ÿ±ÿØ€åŸÅ √ó {total_columns} ÿ≥ÿ™ŸàŸÜ")
        print(f"   üìä ⁄©ŸÑ ÿ≥ŸÑŸàŸÑ‚ÄåŸáÿß: {total_cells:,} ({capacity:.1f}%)")
        print(f"   üîó {file_url}")
        
        message = f"‚úÖ {updated_rows} ÿ±ÿØ€åŸÅ ÿ¨ÿØ€åÿØ | ÿ¨ŸÖÿπ: {total_rows} ÿ±ÿØ€åŸÅ | {total_columns} ÿ≥ÿ™ŸàŸÜ"
        return True, message, file_url, total_rows
        
    except Exception as e:
        print(f"   ‚ùå ÿÆÿ∑ÿß: {e}")
        import traceback
        traceback.print_exc()
        return False, str(e), None, 0

def get_or_create_folder(folder_name="Exhibition_Data"):
    """find or create folder in drive"""
    try:
        drive_service, _ = get_google_services()
        if not drive_service:
            return None
        
        query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
        results = drive_service.files().list(
            q=query, spaces='drive', fields='files(id, name)', pageSize=1
        ).execute()
        files = results.get('files', [])
        
        if files:
            print(f"   ‚úÖ ŸæŸàÿ¥Ÿá ŸÖŸàÿ¨ŸàÿØ: {files[0]['name']}")
            return files[0]['id']
        
        folder = drive_service.files().create(
            body={'name': folder_name, 'mimeType': 'application/vnd.google-apps.folder'},
            fields='id'
        ).execute()
        print(f"   ‚úÖ ŸæŸàÿ¥Ÿá ÿ¨ÿØ€åÿØ: {folder_name}")
        return folder.get('id')
        
    except Exception as e:
        print(f"   ‚ùå ÿÆÿ∑ÿß: {e}")
        return None



# =========================================================
# Quota Management
# =========================================================
DAILY_LIMIT = 240
QUOTA_FILE = Path("quota.json")

def save_quota(q):
    QUOTA_FILE.write_text(json.dumps(q, indent=2, ensure_ascii=False), encoding="utf-8")

def load_quota():
    today = datetime.date.today().isoformat()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            file_date = data.get("date")
            if file_date != today:
                q = {"date": today, "used": 0, "remaining": DAILY_LIMIT}
                save_quota(q)
                return q
            used = data.get("used", 0)
            remaining = max(0, DAILY_LIMIT - used)
            q = {"date": today, "used": used, "remaining": remaining}
            save_quota(q)
            return q
        except Exception:
            pass
    q = {"date": today, "used": 0, "remaining": DAILY_LIMIT}
    save_quota(q)
    return q

def decrease_quota(amount=1):
    quota = load_quota()
    quota["used"] += amount
    quota["remaining"] = max(0, DAILY_LIMIT - quota["used"])
    save_quota(quota)
    return quota


# =========================================================
# Quality Control Tracking Functions
# =========================================================
def get_qc_metadata(user_name, user_role):
    """create quality control metadata"""
    now = datetime.datetime.now()
    return {
        "QC_Supervisor": user_name,
        "QC_Role": user_role,
        "QC_Date": now.strftime("%Y-%m-%d"),
        "QC_Time": now.strftime("%H:%M:%S"),
        "QC_Timestamp": now.strftime("%Y-%m-%d %H:%M:%S")
    }

def add_qc_metadata_to_excel(excel_path, qc_metadata):
    """add quality control metadata to excel"""
    try:
        df = pd.read_excel(excel_path)
        for key in ["QC_Supervisor", "QC_Role", "QC_Date", "QC_Time", "QC_Timestamp"]:
            if key in qc_metadata:
                df.insert(0, key, qc_metadata[key])
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"   ‚úÖ QC Metadata added: {qc_metadata['QC_Supervisor']} ({qc_metadata['QC_Role']})")
        return True
    except Exception as e:
        print(f"   ‚ùå Error adding QC metadata: {e}")
        return False

def save_qc_log(session_dir, qc_metadata, exhibition_name, pipeline_type, total_files):
    """save quality control log to a json file"""
    try:
        qc_log_file = session_dir / "qc_log.json"
        qc_log = {
            **qc_metadata,
            "Exhibition": exhibition_name,
            "Pipeline_Type": pipeline_type,
            "Total_Files": total_files,
            "Session_Dir": str(session_dir)
        }
        qc_log_file.write_text(json.dumps(qc_log, indent=2, ensure_ascii=False), encoding="utf-8")
        print(f"   ‚úÖ QC Log saved: {qc_log_file}")
        return True
    except Exception as e:
        print(f"   ‚ùå Error saving QC log: {e}")
        return False


# =========================================================
# shared smart functions
# =========================================================
def detect_source_type(file_name):
    if not file_name or pd.isna(file_name):
        return "Unknown"
    file_name = str(file_name).lower()
    if file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.gif')):
        return "Image"
    elif file_name.endswith('.pdf'):
        return "PDF"
    elif file_name.endswith(('.xlsx', '.xls', '.csv')):
        return "Excel"
    else:
        return "Unknown"

def smart_position_from_department(department):
    if not department or pd.isna(department) or str(department).strip() == '':
        return None
    department = str(department).strip().lower()
    department_position_map = {
        'ŸÅÿ±Ÿàÿ¥': 'ŸÖÿØ€åÿ± ŸÅÿ±Ÿàÿ¥', 'sales': 'ŸÖÿØ€åÿ± ŸÅÿ±Ÿàÿ¥',
        'ÿ®ÿßÿ≤ÿßÿ±€åÿßÿ®€å': 'ŸÖÿØ€åÿ± ÿ®ÿßÿ≤ÿßÿ±€åÿßÿ®€å', 'marketing': 'ŸÖÿØ€åÿ± ÿ®ÿßÿ≤ÿßÿ±€åÿßÿ®€å',
        'ÿµÿßÿØÿ±ÿßÿ™': 'ŸÖÿØ€åÿ± ÿµÿßÿØÿ±ÿßÿ™', 'export': 'ŸÖÿØ€åÿ± ÿµÿßÿØÿ±ÿßÿ™',
        'Ÿàÿßÿ±ÿØÿßÿ™': 'ŸÖÿØ€åÿ± Ÿàÿßÿ±ÿØÿßÿ™', 'import': 'ŸÖÿØ€åÿ± Ÿàÿßÿ±ÿØÿßÿ™',
        'ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜ€å': 'ŸÖÿØ€åÿ± ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜ€å', 'commerce': 'ŸÖÿØ€åÿ± ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜ€å',
        'ŸÖÿØ€åÿ±€åÿ™': 'ŸÖÿØ€åÿ±ÿπÿßŸÖŸÑ', 'management': 'ŸÖÿØ€åÿ±ÿπÿßŸÖŸÑ',
        'ÿßÿ¨ÿ±ÿß€å€å': 'ŸÖÿØ€åÿ± ÿßÿ¨ÿ±ÿß€å€å', 'executive': 'ŸÖÿØ€åÿ± ÿßÿ¨ÿ±ÿß€å€å',
        'ÿπÿßŸÖŸÑ': 'ŸÖÿØ€åÿ±ÿπÿßŸÖŸÑ', 'ceo': 'ŸÖÿØ€åÿ±ÿπÿßŸÖŸÑ',
        'ÿ™ŸàŸÑ€åÿØ': 'ŸÖÿØ€åÿ± ÿ™ŸàŸÑ€åÿØ', 'production': 'ŸÖÿØ€åÿ± ÿ™ŸàŸÑ€åÿØ',
        '⁄©ÿßÿ±ÿÆÿßŸÜŸá': 'ŸÖÿØ€åÿ± ⁄©ÿßÿ±ÿÆÿßŸÜŸá', 'factory': 'ŸÖÿØ€åÿ± ⁄©ÿßÿ±ÿÆÿßŸÜŸá',
        'ÿπŸÖŸÑ€åÿßÿ™': 'ŸÖÿØ€åÿ± ÿπŸÖŸÑ€åÿßÿ™', 'operations': 'ŸÖÿØ€åÿ± ÿπŸÖŸÑ€åÿßÿ™',
        'ŸÅŸÜ€å': 'ŸÖÿØ€åÿ± ŸÅŸÜ€å', 'technical': 'ŸÖÿØ€åÿ± ŸÅŸÜ€å',
        'ŸÖÿßŸÑ€å': 'ŸÖÿØ€åÿ± ŸÖÿßŸÑ€å', 'finance': 'ŸÖÿØ€åÿ± ŸÖÿßŸÑ€å',
        'ÿ≠ÿ≥ÿßÿ®ÿØÿßÿ±€å': 'ŸÖÿØ€åÿ± ÿ≠ÿ≥ÿßÿ®ÿØÿßÿ±€å', 'accounting': 'ŸÖÿØ€åÿ± ÿ≠ÿ≥ÿßÿ®ÿØÿßÿ±€å',
        'ŸÖŸÜÿßÿ®ÿπ ÿßŸÜÿ≥ÿßŸÜ€å': 'ŸÖÿØ€åÿ± ŸÖŸÜÿßÿ®ÿπ ÿßŸÜÿ≥ÿßŸÜ€å', 'hr': 'ŸÖÿØ€åÿ± ŸÖŸÜÿßÿ®ÿπ ÿßŸÜÿ≥ÿßŸÜ€å',
        'ŸÅŸÜÿßŸàÿ±€å': 'ŸÖÿØ€åÿ± ŸÅŸÜÿßŸàÿ±€å ÿßÿ∑ŸÑÿßÿπÿßÿ™', 'it': 'ŸÖÿØ€åÿ± IT',
        'ÿ™ÿ≠ŸÇ€åŸÇ': 'ŸÖÿØ€åÿ± ÿ™ÿ≠ŸÇ€åŸÇ Ÿà ÿ™Ÿàÿ≥ÿπŸá', 'r&d': 'ŸÖÿØ€åÿ± R&D',
        '⁄©€åŸÅ€åÿ™': 'ŸÖÿØ€åÿ± ⁄©ŸÜÿ™ÿ±ŸÑ ⁄©€åŸÅ€åÿ™', 'qc': 'ŸÖÿØ€åÿ± ⁄©ŸÜÿ™ÿ±ŸÑ ⁄©€åŸÅ€åÿ™',
        'ÿÆÿØŸÖÿßÿ™': 'ŸÖÿØ€åÿ± ÿÆÿØŸÖÿßÿ™', 'support': 'ŸÖÿØ€åÿ± Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å',
        'ŸÑÿ¨ÿ≥ÿ™€å⁄©': 'ŸÖÿØ€åÿ± ŸÑÿ¨ÿ≥ÿ™€å⁄©', 'logistics': 'ŸÖÿØ€åÿ± ŸÑÿ¨ÿ≥ÿ™€å⁄©',
        'ÿßŸÜÿ®ÿßÿ±': 'ŸÖÿØ€åÿ± ÿßŸÜÿ®ÿßÿ±', 'warehouse': 'ŸÖÿØ€åÿ± ÿßŸÜÿ®ÿßÿ±',
        'ÿÆÿ±€åÿØ': 'ŸÖÿØ€åÿ± ÿÆÿ±€åÿØ', 'purchasing': 'ŸÖÿØ€åÿ± ÿÆÿ±€åÿØ',
        'ÿ±Ÿàÿßÿ®ÿ∑ ÿπŸÖŸàŸÖ€å': 'ŸÖÿØ€åÿ± ÿ±Ÿàÿßÿ®ÿ∑ ÿπŸÖŸàŸÖ€å', 'pr': 'ŸÖÿØ€åÿ± ÿ±Ÿàÿßÿ®ÿ∑ ÿπŸÖŸàŸÖ€å',
    }
    for key, position in department_position_map.items():
        if key in department:
            return position
    if any(word in department for word in ['ŸÖÿØ€åÿ±', 'manager', 'ÿ±ÿ¶€åÿ≥', 'chief']):
        return f"ŸÖÿØ€åÿ± {department.title()}"
    elif any(word in department for word in ['ŸÖÿπÿßŸàŸÜ', 'deputy']):
        return f"ŸÖÿπÿßŸàŸÜ {department.title()}"
    elif any(word in department for word in ['⁄©ÿßÿ±ÿ¥ŸÜÿßÿ≥', 'expert']):
        return f"⁄©ÿßÿ±ÿ¥ŸÜÿßÿ≥ {department.title()}"
    return f"ŸÖÿ≥ÿ¶ŸàŸÑ {department.title()}"

def add_exhibition_and_source(excel_path, exhibition_name):
    """unified version + ui notifications"""
    try:
        print(f"\nüìù Adding Exhibition & Source metadata...")
        df = pd.read_excel(excel_path)
        print(f"   ‚úì Loaded: {len(df)} rows √ó {len(df.columns)} columns")

        df.insert(0, 'Exhibition', exhibition_name)
        if 'file_name' in df.columns:
            df.insert(1, 'Source', df['file_name'].apply(detect_source_type))
        elif 'url' in df.columns or 'Website' in df.columns:
            df.insert(1, 'Source', 'Excel')
        else:
            df.insert(1, 'Source', 'Unknown')

        if 'Department' in df.columns and 'PositionFA' in df.columns:
            print(f"\nü§ñ Smart Position Detection...")
            filled_count = 0
            for idx in df.index:
                if pd.isna(df.loc[idx, 'PositionFA']) or str(df.loc[idx, 'PositionFA']).strip() == '':
                    department = df.loc[idx, 'Department']
                    smart_position = smart_position_from_department(department)
                    if smart_position:
                        df.loc[idx, 'PositionFA'] = smart_position
                        filled_count += 1
                        print(f"   ‚úì Row {idx + 1}: {department} ‚Üí {smart_position}")
            if filled_count > 0:
                st.info(f"ü§ñ Ÿæÿ± ÿ¥ÿØ {filled_count} ÿ≥ŸÖÿ™ ÿßÿ≤ ÿ±Ÿà€å ÿØŸæÿßÿ±ÿ™ŸÖÿßŸÜ")

        columns_to_remove = ['CompanyNameFA_translated']
        removed = 0
        for col in columns_to_remove:
            if col in df.columns:
                df.drop(col, axis=1, inplace=True)
                removed += 1
                print(f"   üóëÔ∏è Removed column: {col}")
        if removed:
            print(f"   ‚úÖ Removed {removed} unnecessary columns")

        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(str)
                    df[col] = df[col].replace('nan', None).replace('', None)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Warning: Could not convert column {col}: {e}")

        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"   ‚úÖ Updated: {excel_path}")
        print(f"   üìä Final: {len(df)} rows √ó {len(df.columns)} columns")
        return True
    except Exception as e:
        print(f"   ‚ùå Error adding metadata: {e}")
        st.error(f"ÿÆÿ∑ÿß ÿØÿ± ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ŸÖÿ™ÿßÿØ€åÿ™ÿß: {e}")
        return False


# =========================================================
# detect pipeline type and exhibition name
# =========================================================
def detect_pipeline_type(files):
    extensions = [f.name.split('.')[-1].lower() for f in files]
    if any(ext in ['xlsx', 'xls'] for ext in extensions):
        return 'excel'
    elif any(ext in ['pdf', 'jpg', 'jpeg', 'png'] for ext in extensions):
        return 'ocr_qr'
    return None

def extract_exhibition_name(files):
    if not files:
        return "Unknown_Exhibition"
    first_file = files[0].name
    name_without_ext = first_file.rsplit('.', 1)[0]
    name_parts = re.split(r'[_\-\s]+', name_without_ext)
    cleaned_parts = [p for p in name_parts if not p.isdigit() and len(p) > 2]
    if cleaned_parts:
        return " ".join(cleaned_parts[:3])
    return "Unknown_Exhibition"

# =========================================================
# Batch Processing Logic
# =========================================================
def get_batch_size(file_type):
    """set batch size based on file type"""
    file_type = file_type.lower()
    if file_type in ['jpg', 'jpeg', 'png', 'bmp', 'webp', 'gif']:
        return 5
    elif file_type == 'pdf':
        return 4
    elif file_type in ['xlsx', 'xls']:
        return 1
    else:
        return 1

def create_batches(files_list, batch_size):
    """split file list into smaller batches"""
    batches = []
    for i in range(0, len(files_list), batch_size):
        batches.append(files_list[i:i + batch_size])
    return batches

def process_files_in_batches(uploads_dir, pipeline_type):
    """Ÿæÿ±ÿØÿßÿ≤ÿ¥ ŸÅÿß€åŸÑ‚ÄåŸáÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ Batch"""
    if pipeline_type == 'excel':
        excel_files = list(uploads_dir.glob("*.xlsx")) + list(uploads_dir.glob("*.xls"))
        return [(f,) for f in excel_files], 1
    
    elif pipeline_type == 'ocr_qr':
        image_files = []
        pdf_files = []
        
        for f in uploads_dir.iterdir():
            if f.is_file():
                ext = f.suffix.lower()
                if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.webp', '.gif']:
                    image_files.append(f)
                elif ext == '.pdf':
                    pdf_files.append(f)
        
        image_batches = create_batches(image_files, 5) if image_files else []
        pdf_batches = create_batches(pdf_files, 4) if pdf_files else []
        all_batches = image_batches + pdf_batches
        
        if image_files and pdf_files:
            avg_batch_size = (5 + 4) / 2
        elif image_files:
            avg_batch_size = 5
        elif pdf_files:
            avg_batch_size = 4
        else:
            avg_batch_size = 1
        
        return all_batches, int(avg_batch_size)
    
    return [], 1